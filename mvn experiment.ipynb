{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math, os, sys, torch, pyro, pyro.optim, pyro.infer\n",
    "import numpy as np\n",
    "from torch.autograd import Variable, grad, Function\n",
    "from torch.autograd.function import once_differentiable\n",
    "from pyro.distributions import Distribution\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BivariateNormal(Distribution):\n",
    "    reparameterized = True\n",
    "    \n",
    "    def __init__(self, loc, scale_triu, batch_size=None):\n",
    "        self.loc = loc\n",
    "        self.scale_triu = scale_triu\n",
    "        self.batch_size = 1 if batch_size is None else batch_size\n",
    "\n",
    "    def batch_shape(self, x=None):\n",
    "        loc = self.loc.expand(self.batch_size, *self.loc.size()).squeeze(0)\n",
    "        if x is not None:\n",
    "            if x.size()[-1] != loc.size()[-1]:\n",
    "                raise ValueError(\"The event size for the data and distribution parameters must match.\\n\"\n",
    "                                 \"Expected x.size()[-1] == self.loc.size()[0], but got {} vs {}\".format(\n",
    "                                     x.size(-1), loc.size(-1)))\n",
    "            try:\n",
    "                loc = loc.expand_as(x)\n",
    "            except RuntimeError as e:\n",
    "                raise ValueError(\"Parameter `loc` with shape {} is not broadcastable to \"\n",
    "                                 \"the data shape {}. \\nError: {}\".format(loc.size(), x.size(), str(e)))\n",
    "\n",
    "        return loc.size()[:-1]\n",
    "\n",
    "    def event_shape(self):\n",
    "        return self.loc.size()[-1:]\n",
    "\n",
    "    def sample(self):\n",
    "        return self.loc + torch.mv(self.scale_triu.t(), Variable(torch.randn(self.loc.size())), )\n",
    "\n",
    "    def batch_log_pdf(self, x):\n",
    "        delta = x - self.loc\n",
    "        z0 = delta[..., 0] / self.scale_triu[..., 0, 0]\n",
    "        z1 = (delta[..., 1] - self.scale_triu[..., 0, 1] * z0) / self.scale_triu[..., 1, 1]\n",
    "        z = torch.stack([z0, z1], dim=-1)\n",
    "        mahalanobis_squared = (z ** 2).sum(-1)\n",
    "        normalization_constant = self.scale_triu.diag().log().sum(-1) + np.log(2 * np.pi)\n",
    "        return -(normalization_constant + 0.5 * mahalanobis_squared).unsqueeze(-1)\n",
    "\n",
    "    def entropy(self):\n",
    "        return self.scale_triu.diag().log().sum() + (1 + math.log(2 * math.pi))\n",
    "    \n",
    "def _BVN_backward_reptrick(white, scale_triu, grad_output):   \n",
    "    grad = (grad_output.unsqueeze(-1) * white.unsqueeze(-2)).squeeze(0)  \n",
    "    #print(\"grad_output\", grad_output)\n",
    "    #print(\"torch.triu(grad)\", torch.triu(grad))\n",
    "    return grad_output, torch.triu(grad)\n",
    "        \n",
    "def _BVN_backward_symm(white, scale_triu, grad_output):\n",
    "    grad = (grad_output.unsqueeze(-1) * white.unsqueeze(-2)).squeeze(0)\n",
    "    x = torch.trtrs(white.t().data, scale_triu.data, transpose=False)[0].t()\n",
    "    x = Variable(x)\n",
    "    #print(\"x\",x)\n",
    "    y = torch.mm(scale_triu, grad_output.t()).t()\n",
    "    #print(\"y\", y)\n",
    "    grad += (x.unsqueeze(-1) * y.unsqueeze(-2)).squeeze(0)\n",
    "    grad *= 0.5\n",
    "    #print(\"grad_output\", grad_output)\n",
    "    #print(\"torch.triu(grad)\", torch.triu(grad)) \n",
    "    return grad_output, torch.triu(grad.t())\n",
    "    \n",
    "class _RepTrickSample(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, loc, scale_triu):\n",
    "        ctx.save_for_backward(scale_triu)\n",
    "        ctx.white = loc.new(loc.size()).normal_()\n",
    "        return loc + torch.mm(ctx.white, scale_triu)\n",
    " \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        scale_triu, = ctx.saved_variables\n",
    "        return _BVN_backward_reptrick(Variable(ctx.white), scale_triu, grad_output)    \n",
    "\n",
    "class _SymmetricSample(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, loc, scale_triu):\n",
    "        ctx.save_for_backward(scale_triu)\n",
    "        ctx.white = loc.new(loc.size()).normal_()\n",
    "        return loc + torch.mm(ctx.white, scale_triu)\n",
    " \n",
    "    @staticmethod\n",
    "    @once_differentiable    \n",
    "    def backward(ctx, grad_output):\n",
    "        scale_triu, = ctx.saved_variables\n",
    "        return _BVN_backward_symm(Variable(ctx.white), scale_triu, Variable(grad_output))    \n",
    "\n",
    "\n",
    "class BivariateNormalRepTrick(BivariateNormal):\n",
    "    def sample(self):\n",
    "        loc = self.loc.expand(self.batch_size, *self.loc.size())\n",
    "        return _RepTrickSample.apply(loc, self.scale_triu)\n",
    "\n",
    "class BivariateNormalSymmetric(BivariateNormal):\n",
    "    def sample(self):\n",
    "        loc = self.loc.expand(self.batch_size, *self.loc.size())\n",
    "        return _SymmetricSample.apply(loc, self.scale_triu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = Variable(torch.zeros(2))\n",
    "scale_triu = Variable(torch.Tensor([[1, 0], [0, 1]]), requires_grad=True)\n",
    "distrt = BivariateNormalRepTrick(mu, scale_triu)\n",
    "distsym = BivariateNormalSymmetric(mu, scale_triu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-2.6155 -1.5268\n",
      " 0.0000 -0.5203\n",
      "[torch.DoubleTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = distrt.sample()\n",
    "torch.pow(z,3.0).sum().backward()\n",
    "print(scale_triu.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Variable data has to be a tensor, but got Variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-afffb95dc471>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistsym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale_triu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/torch/autograd/function.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/torch/autograd/function.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0merr_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         return err_fn(*[Variable(o, **kwargs) if o is not None else None\n\u001b[0;32m--> 229\u001b[0;31m                       for o in outputs])\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Variable data has to be a tensor, but got Variable"
     ]
    }
   ],
   "source": [
    "z = distsym.sample()\n",
    "torch.pow(z,3.0).sum().backward()\n",
    "print(scale_triu.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
