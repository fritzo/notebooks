{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, os, sys, torch, pyro, pyro.optim, pyro.infer\n",
    "import numpy as np\n",
    "from torch.autograd import Variable, grad, Function\n",
    "from torch.autograd.function import once_differentiable\n",
    "from pyro.distributions import Distribution\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BivariateNormal(Distribution):\n",
    "    reparameterized = True\n",
    "    \n",
    "    def __init__(self, loc, scale_triu, batch_size=None):\n",
    "        self.loc = loc\n",
    "        self.scale_triu = scale_triu\n",
    "        self.batch_size = 1 if batch_size is None else batch_size\n",
    "\n",
    "    def batch_shape(self, x=None):\n",
    "        loc = self.loc.expand(self.batch_size, *self.loc.size()).squeeze(0)\n",
    "        if x is not None:\n",
    "            if x.size()[-1] != loc.size()[-1]:\n",
    "                raise ValueError(\"The event size for the data and distribution parameters must match.\\n\"\n",
    "                                 \"Expected x.size()[-1] == self.loc.size()[0], but got {} vs {}\".format(\n",
    "                                     x.size(-1), loc.size(-1)))\n",
    "            try:\n",
    "                loc = loc.expand_as(x)\n",
    "            except RuntimeError as e:\n",
    "                raise ValueError(\"Parameter `loc` with shape {} is not broadcastable to \"\n",
    "                                 \"the data shape {}. \\nError: {}\".format(loc.size(), x.size(), str(e)))\n",
    "\n",
    "        return loc.size()[:-1]\n",
    "\n",
    "    def event_shape(self):\n",
    "        return self.loc.size()[-1:]\n",
    "\n",
    "    def sample(self):\n",
    "        return self.loc + torch.mv(self.scale_triu.t(), Variable(torch.randn(self.loc.size())), )\n",
    "\n",
    "    def batch_log_pdf(self, x):\n",
    "        delta = x - self.loc\n",
    "        z0 = delta[..., 0] / self.scale_triu[..., 0, 0]\n",
    "        z1 = (delta[..., 1] - self.scale_triu[..., 0, 1] * z0) / self.scale_triu[..., 1, 1]\n",
    "        z = torch.stack([z0, z1], dim=-1)\n",
    "        mahalanobis_squared = (z ** 2).sum(-1)\n",
    "        normalization_constant = self.scale_triu.diag().log().sum(-1) + np.log(2 * np.pi)\n",
    "        return -(normalization_constant + 0.5 * mahalanobis_squared).unsqueeze(-1)\n",
    "\n",
    "    def entropy(self):\n",
    "        return self.scale_triu.diag().log().sum() + (1 + math.log(2 * math.pi))\n",
    "    \n",
    "def _BVN_backward_reptrick(white, scale_triu, grad_output):   \n",
    "    grad = (grad_output.unsqueeze(-1) * white.unsqueeze(-2)).squeeze(0)  \n",
    "    #print(\"grad_output\", grad_output)\n",
    "    #print(\"torch.triu(grad)\", torch.triu(grad))\n",
    "    return grad_output, torch.triu(grad)\n",
    "        \n",
    "def _BVN_backward_symm(white, scale_triu, grad_output):\n",
    "    grad = (grad_output.unsqueeze(-1) * white.unsqueeze(-2)).squeeze(0)\n",
    "    x = torch.trtrs(white.t(), scale_triu, transpose=False)[0].t()\n",
    "    #print(\"x\",x)\n",
    "    y = torch.mm(scale_triu, grad_output.t()).t()\n",
    "    #print(\"y\", y)\n",
    "    grad += (x.unsqueeze(-1) * y.unsqueeze(-2)).squeeze(0)\n",
    "    grad *= 0.5\n",
    "    #print(\"grad_output\", grad_output)\n",
    "    #print(\"torch.triu(grad)\", torch.triu(grad)) \n",
    "    return grad_output, torch.triu(grad.t())\n",
    "    \n",
    "class _RepTrickSample(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, loc, scale_triu):\n",
    "        ctx.save_for_backward(scale_triu)\n",
    "        ctx.white = loc.new(loc.size()).normal_()\n",
    "        return loc + torch.mm(ctx.white, scale_triu)\n",
    " \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        scale_triu, = ctx.saved_variables\n",
    "        return _BVN_backward_reptrick(Variable(ctx.white), scale_triu, grad_output)    \n",
    "\n",
    "class _SymmetricSample(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, loc, scale_triu):\n",
    "        ctx.save_for_backward(scale_triu)\n",
    "        ctx.white = loc.new(loc.size()).normal_()\n",
    "        return loc + torch.mm(ctx.white, scale_triu)\n",
    " \n",
    "    @staticmethod\n",
    "    @once_differentiable    \n",
    "    def backward(ctx, grad_output):\n",
    "        scale_triu, = ctx.saved_tensors\n",
    "        return _BVN_backward_symm(ctx.white, scale_triu, grad_output)    \n",
    "\n",
    "\n",
    "class BivariateNormalRepTrick(BivariateNormal):\n",
    "    def sample(self):\n",
    "        loc = self.loc.expand(self.batch_size, *self.loc.size())\n",
    "        return _RepTrickSample.apply(loc, self.scale_triu)\n",
    "\n",
    "class BivariateNormalSymmetric(BivariateNormal):\n",
    "    def sample(self):\n",
    "        loc = self.loc.expand(self.batch_size, *self.loc.size())\n",
    "        return _SymmetricSample.apply(loc, self.scale_triu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = Variable(torch.zeros(2))\n",
    "scale_triu = Variable(torch.Tensor([[1, 0], [0, 1]]), requires_grad=True)\n",
    "distrt = BivariateNormalRepTrick(mu, scale_triu)\n",
    "distsym = BivariateNormalSymmetric(mu, scale_triu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "  0.0000  -0.0000\n",
      "  0.0000 -10.1429\n",
      "[torch.DoubleTensor of size (2,2)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = distrt.sample()\n",
    "torch.pow(z,3.0).sum().backward()\n",
    "print(scale_triu.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.0217  0.4035\n",
      " 0.0000 -6.2952\n",
      "[torch.DoubleTensor of size (2,2)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = distsym.sample()\n",
    "torch.pow(z,3.0).sum().backward()\n",
    "print(scale_triu.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
