{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preserving sparsity of matrix inverse square root\n",
    "\n",
    "The posterior joint distribution of a Gaussian probabilistic graphical models has a sparse precision matrix $P$, with sparsity structure determined by the dependency graph. In plated graphical models, the sparsity structure is blockwise, allowing within-block use of fast dense linear algebra operations on CPU and GPU. During Bayesian inference, we need to compute the inverse square root matrix $P^{-1/2}$, or at least compute a Cholesky factor $P^{1/2}$ and solve $P^{1/2} \\backslash z$ for white noise vectors $z$.\n",
    "\n",
    "This notebook explores sparsity preserving representations of precision matrices, Cholesky factors, and Cholesky factor inverses. By contrast, a naive Cholesky decomposition does not preserve sparsity structure, leading to so-called fill-in elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(20210920)\n",
    "torch.set_default_dtype(torch.double)\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a representation that decomposes a Cholesky factor into a product of lower-triangular matrices each of which is the identity on all except one row, e.g.\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "a & 0 & 0 \\\\ b & c & 0 \\\\ d & e & f\n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ d & e & f\n",
    "\\end{pmatrix}\n",
    "\\times\n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\ b & c & 0 \\\\ 0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "\\times\n",
    "\\begin{pmatrix}\n",
    "a & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangular_decompose(L):\n",
    "    assert L.size(-1) == L.size(-2)\n",
    "    assert (L == L.tril()).all()\n",
    "    N, N = L.shape\n",
    "    factors = []\n",
    "    for i, row in enumerate(L):\n",
    "        Li = torch.eye(N)\n",
    "        Li[i] = L[i]\n",
    "        factors.append(Li)\n",
    "    return factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L:\n",
      "[[-1.061  0.     0.   ]\n",
      " [-0.668  1.006  0.   ]\n",
      " [-0.055 -0.861  0.878]]\n",
      "L_0\n",
      "[[-1.061  0.     0.   ]\n",
      " [ 0.     1.     0.   ]\n",
      " [ 0.     0.     1.   ]]\n",
      "L_1\n",
      "[[ 1.     0.     0.   ]\n",
      " [-0.668  1.006  0.   ]\n",
      " [ 0.     0.     1.   ]]\n",
      "L_2\n",
      "[[ 1.     0.     0.   ]\n",
      " [ 0.     1.     0.   ]\n",
      " [-0.055 -0.861  0.878]]\n",
      "product(factors):\n",
      "[[-1.061  0.     0.   ]\n",
      " [-0.668  1.006  0.   ]\n",
      " [-0.055 -0.861  0.878]]\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "L = torch.randn(N, N).tril_()\n",
    "L.diag().exp_()\n",
    "print(\"L:\", L.numpy(), sep=\"\\n\")\n",
    "factors = triangular_decompose(L)\n",
    "for i, Li in enumerate(factors):\n",
    "    print(f\"L_{i}\", Li.numpy(), sep=\"\\n\")\n",
    "prod = reduce(operator.matmul, factors)\n",
    "assert torch.allclose(prod, L)\n",
    "print(\"product(factors):\", prod.numpy(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now note that each factor can be inverted and/or squared while preserving sparsity structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L:\n",
      "[[ 1.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     1.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     1.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     1.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     1.     0.     0.     0.   ]\n",
      " [-0.076 -1.178 -1.087  1.198  0.158  1.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     1.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     1.   ]]\n",
      "L.inverse():\n",
      "[[ 1.     0.     0.     0.     0.     0.     0.    -0.   ]\n",
      " [ 0.     1.     0.     0.     0.     0.     0.    -0.   ]\n",
      " [ 0.     0.     1.     0.     0.     0.     0.    -0.   ]\n",
      " [ 0.     0.     0.     1.    -0.     0.     0.    -0.   ]\n",
      " [ 0.     0.     0.     0.     1.     0.     0.    -0.   ]\n",
      " [ 0.076  1.178  1.087 -1.198 -0.158  1.     0.    -0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     1.    -0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     1.   ]]\n",
      "L @ L.T:\n",
      "[[ 1.     0.     0.     0.     0.    -0.076  0.     0.   ]\n",
      " [ 0.     1.     0.     0.     0.    -1.178  0.     0.   ]\n",
      " [ 0.     0.     1.     0.     0.    -1.087  0.     0.   ]\n",
      " [ 0.     0.     0.     1.     0.     1.198  0.     0.   ]\n",
      " [ 0.     0.     0.     0.     1.     0.158  0.     0.   ]\n",
      " [-0.076 -1.178 -1.087  1.198  0.158  5.035  0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     1.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     1.   ]]\n"
     ]
    }
   ],
   "source": [
    "N = 8\n",
    "R = 5  # row\n",
    "L = torch.eye(N)\n",
    "L[R, :R].normal_()\n",
    "L[R, R].abs_()\n",
    "print(\"L:\", L.numpy(), sep=\"\\n\")\n",
    "print(\"L.inverse():\", L.inverse().numpy(), sep=\"\\n\")\n",
    "print(\"L @ L.T:\", (L @ L.T).numpy(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L:\n",
      "[[ 1.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     1.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     1.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     1.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     1.     0.     0.     0.   ]\n",
      " [ 0.     0.    -1.819 -0.3    0.819  1.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     1.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     1.   ]]\n",
      "L.inverse():\n",
      "[[ 1.     0.     0.     0.     0.     0.     0.    -0.   ]\n",
      " [ 0.     1.     0.     0.     0.     0.     0.    -0.   ]\n",
      " [ 0.     0.     1.     0.     0.     0.     0.    -0.   ]\n",
      " [ 0.     0.     0.     1.     0.     0.     0.    -0.   ]\n",
      " [ 0.     0.     0.     0.     1.     0.     0.    -0.   ]\n",
      " [ 0.     0.     1.819  0.3   -0.819  1.     0.    -0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     1.    -0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     1.   ]]\n",
      "L @ L.T:\n",
      "[[ 1.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     1.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     1.     0.     0.    -1.819  0.     0.   ]\n",
      " [ 0.     0.     0.     1.     0.    -0.3    0.     0.   ]\n",
      " [ 0.     0.     0.     0.     1.     0.819  0.     0.   ]\n",
      " [ 0.     0.    -1.819 -0.3    0.819  5.068  0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     1.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     1.   ]]\n"
     ]
    }
   ],
   "source": [
    "L = torch.eye(N)\n",
    "L[R, 2:R].normal_()\n",
    "L[R, R].abs_()\n",
    "print(\"L:\", L.numpy(), sep=\"\\n\")\n",
    "print(\"L.inverse():\", L.inverse().numpy(), sep=\"\\n\")\n",
    "print(\"L @ L.T:\", (L @ L.T).numpy(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L:\n",
      "[[ 1.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     1.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     1.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     1.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     1.     0.     0.     0.   ]\n",
      " [-0.728 -0.44   0.     0.     0.     0.54   0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     1.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     1.   ]]\n",
      "L.inverse():\n",
      "[[ 1.     0.     0.     0.     0.     0.     0.    -0.   ]\n",
      " [ 0.     1.     0.     0.     0.     0.     0.    -0.   ]\n",
      " [ 0.     0.     1.     0.     0.     0.     0.    -0.   ]\n",
      " [ 0.     0.     0.     1.     0.     0.     0.    -0.   ]\n",
      " [ 0.     0.     0.     0.     1.     0.     0.    -0.   ]\n",
      " [ 1.348  0.815  0.     0.     0.     1.852  0.    -0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     1.    -0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     1.   ]]\n",
      "L @ L.T:\n",
      "[[ 1.     0.     0.     0.     0.    -0.728  0.     0.   ]\n",
      " [ 0.     1.     0.     0.     0.    -0.44   0.     0.   ]\n",
      " [ 0.     0.     1.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     1.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     1.     0.     0.     0.   ]\n",
      " [-0.728 -0.44   0.     0.     0.     1.016  0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     1.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     1.   ]]\n"
     ]
    }
   ],
   "source": [
    "L = torch.eye(N)\n",
    "L[R, :2].normal_()\n",
    "L[R, R].normal_().abs_()\n",
    "print(\"L:\", L.numpy(), sep=\"\\n\")\n",
    "print(\"L.inverse():\", L.inverse().numpy(), sep=\"\\n\")\n",
    "print(\"L @ L.T:\", (L @ L.T).numpy(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next consider blockwise sparsity structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangular_decompose_block(L, B):\n",
    "    assert L.size(-1) == L.size(-2)\n",
    "    assert (L == L.tril()).all()\n",
    "    N, N = L.shape\n",
    "    assert N % B == 0\n",
    "    factors = []\n",
    "    for i in range(N // B):\n",
    "        Li = torch.eye(N)\n",
    "        Li[i * B: (i + 1) * B] = L[i * B: (i + 1) * B]\n",
    "        factors.append(Li)\n",
    "    return factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L:\n",
      "[[ 1.078  0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 1.416 -1.02   0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.871  0.042  1.045  0.     0.     0.     0.     0.     0.   ]\n",
      " [-1.881 -0.185  0.715  0.307  0.     0.     0.     0.     0.   ]\n",
      " [-1.424  0.892 -1.504 -1.043  0.224  0.     0.     0.     0.   ]\n",
      " [ 0.641  0.067 -0.551 -1.975  0.153 -1.462  0.     0.     0.   ]\n",
      " [ 0.577 -0.062  0.688  0.339 -0.068  0.825  1.212  0.     0.   ]\n",
      " [ 0.852  0.013  0.693 -0.594 -0.465 -0.733 -0.366  0.588  0.   ]\n",
      " [ 0.725  0.485 -0.139  0.05   0.902  0.404  0.199 -1.202 -0.567]]\n",
      "L_0\n",
      "[[ 1.078  0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 1.416 -1.02   0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.871  0.042  1.045  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     1.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     1.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     1.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     1.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     1.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     1.   ]]\n",
      "L_1\n",
      "[[ 1.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     1.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     1.     0.     0.     0.     0.     0.     0.   ]\n",
      " [-1.881 -0.185  0.715  0.307  0.     0.     0.     0.     0.   ]\n",
      " [-1.424  0.892 -1.504 -1.043  0.224  0.     0.     0.     0.   ]\n",
      " [ 0.641  0.067 -0.551 -1.975  0.153 -1.462  0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     1.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     1.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     1.   ]]\n",
      "L_2\n",
      "[[ 1.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     1.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     1.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     1.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     1.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     1.     0.     0.     0.   ]\n",
      " [ 0.577 -0.062  0.688  0.339 -0.068  0.825  1.212  0.     0.   ]\n",
      " [ 0.852  0.013  0.693 -0.594 -0.465 -0.733 -0.366  0.588  0.   ]\n",
      " [ 0.725  0.485 -0.139  0.05   0.902  0.404  0.199 -1.202 -0.567]]\n",
      "product(factors):\n",
      "[[ 1.078  0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 1.416 -1.02   0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.871  0.042  1.045  0.     0.     0.     0.     0.     0.   ]\n",
      " [-1.881 -0.185  0.715  0.307  0.     0.     0.     0.     0.   ]\n",
      " [-1.424  0.892 -1.504 -1.043  0.224  0.     0.     0.     0.   ]\n",
      " [ 0.641  0.067 -0.551 -1.975  0.153 -1.462  0.     0.     0.   ]\n",
      " [ 0.577 -0.062  0.688  0.339 -0.068  0.825  1.212  0.     0.   ]\n",
      " [ 0.852  0.013  0.693 -0.594 -0.465 -0.733 -0.366  0.588  0.   ]\n",
      " [ 0.725  0.485 -0.139  0.05   0.902  0.404  0.199 -1.202 -0.567]]\n"
     ]
    }
   ],
   "source": [
    "N = 9\n",
    "B = 3\n",
    "L = torch.randn(N, N).tril_()\n",
    "L.diag().exp_()\n",
    "print(\"L:\", L.numpy(), sep=\"\\n\")\n",
    "factors = triangular_decompose_block(L, 3)\n",
    "for i, Li in enumerate(factors):\n",
    "    print(f\"L_{i}\", Li.numpy(), sep=\"\\n\")\n",
    "prod = reduce(operator.matmul, factors)\n",
    "assert torch.allclose(prod, L)\n",
    "print(\"product(factors):\", prod.numpy(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll see if we can represent a sparse precision matrix as compressed inverse Cholesky factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_precision(size, density):\n",
    "    mask = torch.full((size, size), density).bernoulli_()\n",
    "    x = torch.zeros(size, size)\n",
    "    for i, row in enumerate(mask):\n",
    "        for j, m in enumerate(row[:i].tolist()):\n",
    "            if m:\n",
    "                xij = torch.randn(2, 2)\n",
    "                xij = xij @ xij.T\n",
    "                x[i, i] += xij[0, 0]\n",
    "                x[i, j] += xij[0, 1]\n",
    "                x[j, i] += xij[1, 0]\n",
    "                x[j, j] += xij[1, 1]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:\n",
      "[[ 7.716 -2.364  0.     1.228  0.     0.112  0.     0.753  0.   ]\n",
      " [-2.364 14.85   1.932  0.    -1.255  0.     0.     3.182  0.   ]\n",
      " [ 0.     1.932  8.682  0.841  0.    -0.169  0.     0.187  0.788]\n",
      " [ 1.228  0.     0.841  4.663  0.039  0.    -0.747  0.     0.   ]\n",
      " [ 0.    -1.255  0.     0.039  7.817  0.    -0.586 -0.054  0.   ]\n",
      " [ 0.112  0.    -0.169  0.     0.     5.544  0.     0.273  0.   ]\n",
      " [ 0.     0.     0.    -0.747 -0.586  0.     7.006 -0.306  0.   ]\n",
      " [ 0.753  3.182  0.187  0.    -0.054  0.273 -0.306 11.761  0.422]\n",
      " [ 0.     0.     0.788  0.     0.     0.     0.     0.422  2.606]]\n",
      "Cholesky:\n",
      "[[ 2.778  0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [-0.851  3.758  0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.514  2.901  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.442  0.1    0.272  2.094  0.     0.     0.     0.     0.   ]\n",
      " [ 0.    -0.334  0.059  0.027  2.775  0.     0.     0.     0.   ]\n",
      " [ 0.04   0.009 -0.06  -0.001  0.002  2.353  0.     0.     0.   ]\n",
      " [ 0.     0.     0.    -0.357 -0.208  0.     2.615  0.     0.   ]\n",
      " [ 0.271  0.908 -0.096 -0.088  0.093  0.105 -0.122  3.288  0.   ]\n",
      " [ 0.     0.     0.272 -0.035 -0.005  0.007 -0.005  0.135  1.585]]\n",
      "inv(Cholesky):\n",
      "[[ 0.36   0.     0.     0.     0.     0.     0.     0.    -0.   ]\n",
      " [ 0.082  0.266  0.     0.     0.     0.     0.     0.    -0.   ]\n",
      " [-0.014 -0.047  0.345  0.     0.     0.     0.     0.    -0.   ]\n",
      " [-0.078 -0.007 -0.045  0.478  0.     0.     0.     0.    -0.   ]\n",
      " [ 0.011  0.033 -0.007 -0.005  0.36   0.     0.     0.    -0.   ]\n",
      " [-0.007 -0.002  0.009  0.    -0.     0.425  0.     0.    -0.   ]\n",
      " [-0.01   0.002 -0.007  0.065  0.029 -0.     0.382  0.    -0.   ]\n",
      " [-0.055 -0.076  0.009  0.015 -0.009 -0.014  0.014  0.304 -0.   ]\n",
      " [ 0.005  0.015 -0.061  0.01   0.002 -0.001  0.    -0.026  0.631]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fobermey/opt/miniconda3/envs/pyro/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: torch.cholesky is deprecated in favor of torch.linalg.cholesky and will be removed in a future PyTorch release.\n",
      "L = torch.cholesky(A)\n",
      "should be replaced with\n",
      "L = torch.linalg.cholesky(A)\n",
      "and\n",
      "U = torch.cholesky(A, upper=True)\n",
      "should be replaced with\n",
      "U = torch.linalg.cholesky(A.transpose(-2, -1).conj()).transpose(-2, -1).conj() (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1284.)\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(20210920)\n",
    "precision = make_precision(9, 0.0)\n",
    "print(\"Precision:\", precision.numpy(), sep=\"\\n\")\n",
    "print(\"Cholesky:\", torch.cholesky(precision).numpy(), sep=\"\\n\")\n",
    "print(\"inv(Cholesky):\", torch.cholesky(precision).inverse().numpy(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Cholesky matrix and its inverse do not fully preserve sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress(param):\n",
    "    factors = triangular_decompose(param)\n",
    "    L = reduce(operator.matmul, factors)\n",
    "    cov = L @ L.T\n",
    "    precision = torch.inverse(cov)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 1. 0. 0.]\n",
      " [1. 1. 1. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "mask = (precision != 0).type_as(precision).tril()\n",
    "print(mask.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"
     ]
    }
   ],
   "source": [
    "param = torch.eye(len(precision)).requires_grad_()\n",
    "optim = torch.optim.Adam([param], lr=0.02)\n",
    "for _ in range(1000):\n",
    "    optim.zero_grad()\n",
    "    loss = (decompress(param * mask) - precision).square().sum()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    print(\".\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param:\n",
      "[[ 0.381  0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.07   0.273  0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.    -0.057  0.346  0.     0.     0.     0.     0.     0.   ]\n",
      " [-0.102  0.    -0.06   0.466  0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.042  0.     0.003  0.359  0.     0.     0.     0.   ]\n",
      " [-0.006  0.     0.011  0.     0.     0.425  0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.045  0.03   0.     0.378  0.     0.   ]\n",
      " [-0.043 -0.072 -0.003  0.     0.003 -0.01   0.009  0.292  0.   ]\n",
      " [ 0.     0.    -0.097  0.     0.     0.     0.    -0.04   0.619]]\n",
      "reconstructed precision:\n",
      "tensor([[ 7.7164e+00, -2.3306e+00, -1.0719e-01,  1.2156e+00,  2.1680e-01,\n",
      "          1.0917e-01, -2.0741e-01,  7.4231e-01, -5.6066e-03],\n",
      "        [-2.3306e+00,  1.4863e+01,  1.8746e+00,  1.7613e-01, -1.2131e+00,\n",
      "          3.3626e-02, -1.0802e-02,  3.1688e+00,  2.4833e-01],\n",
      "        [-1.0719e-01,  1.8746e+00,  8.6978e+00,  8.1427e-01,  3.1551e-04,\n",
      "         -1.7449e-01, -1.2120e-01,  2.0820e-01,  7.3557e-01],\n",
      "        [ 1.2156e+00,  1.7613e-01,  8.1427e-01,  4.6755e+00,  9.5296e-03,\n",
      "          6.1404e-04, -6.6844e-01,  2.7364e-02,  8.2699e-04],\n",
      "        [ 2.1680e-01, -1.2131e+00,  3.1551e-04,  9.5296e-03,  7.8180e+00,\n",
      "         -1.7434e-03, -5.8200e-01, -7.7694e-02, -2.3481e-03],\n",
      "        [ 1.0917e-01,  3.3626e-02, -1.7449e-01,  6.1404e-04, -1.7434e-03,\n",
      "          5.5436e+00, -6.2957e-03,  2.6406e-01,  7.9805e-03],\n",
      "        [-2.0741e-01, -1.0802e-02, -1.2120e-01, -6.6844e-01, -5.8200e-01,\n",
      "         -6.2957e-03,  7.0066e+00, -2.8056e-01, -8.4792e-03],\n",
      "        [ 7.4231e-01,  3.1688e+00,  2.0820e-01,  2.7364e-02, -7.7694e-02,\n",
      "          2.6406e-01, -2.8056e-01,  1.1768e+01,  3.5564e-01],\n",
      "        [-5.6066e-03,  2.4833e-01,  7.3557e-01,  8.2699e-04, -2.3481e-03,\n",
      "          7.9805e-03, -8.4792e-03,  3.5564e-01,  2.6058e+00]])\n"
     ]
    }
   ],
   "source": [
    "print(\"param:\", param.data.numpy(), sep=\"\\n\")\n",
    "print(\"reconstructed precision:\", decompress(param.data * mask), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.145  0.027  0.    -0.039  0.    -0.002  0.    -0.016  0.   ]\n",
      " [ 0.027  0.079 -0.016 -0.007  0.011 -0.     0.    -0.023  0.   ]\n",
      " [ 0.    -0.016  0.123 -0.021 -0.002  0.004  0.     0.003 -0.034]\n",
      " [-0.039 -0.007 -0.021  0.231  0.001 -0.     0.021  0.005  0.006]\n",
      " [ 0.     0.011 -0.002  0.001  0.13   0.     0.011 -0.002  0.   ]\n",
      " [-0.002 -0.     0.004 -0.     0.     0.181  0.    -0.004 -0.001]\n",
      " [ 0.     0.     0.     0.021  0.011  0.     0.146  0.003  0.   ]\n",
      " [-0.016 -0.023  0.003  0.005 -0.002 -0.004  0.003  0.093 -0.011]\n",
      " [ 0.     0.    -0.034  0.006  0.    -0.001  0.    -0.011  0.395]]\n"
     ]
    }
   ],
   "source": [
    "print((param @ param.T).data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
